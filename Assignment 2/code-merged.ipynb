{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb074955-7cd4-4887-a14f-0e21b4ec0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will mainly use numpy to construct your NN\n",
    "import numpy as np\n",
    "import matplotlib, time, copy, os, requests, zipfile, sys\n",
    "# Matplotlib to plot the image\n",
    "import matplotlib.pyplot as plt\n",
    "# Off-the-shelf evaluation functions provided by sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bfd87-7ef9-4b7b-a725-4cf0ca91c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onedrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3790cd-0e4f-4971-b71f-90b52ab4fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onedrivedownloader import download\n",
    "\n",
    "link = 'https://unioulu-my.sharepoint.com/:u:/g/personal/hliu22_univ_yo_oulu_fi/EW3HQP_PhE9GpXBS50bADNIBzmmQX2lBpaEp-9mWhpPrsA?e=auRurQ'\n",
    "\n",
    "if not os.path.exists('./data_hw2/fashion_mnist_npy'):\n",
    "    print('Downloading dataset')\n",
    "    download(link, filename=\"./fashion_mnist_npy.zip\", unzip=True, unzip_path='./data_hw2/fashion_mnist_npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffec530-6098-4091-be99-4c760c6d33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_train_data(X):\n",
    "    ''' Input training data has shape (60000, 28, 28)\n",
    "        Input testing data has shape (10000, 28, 28)\n",
    "        where: \n",
    "        60000 is the numbers of input training samples\n",
    "        10000 is the numbers of input testing samples\n",
    "        similar to MNIST, resolution of each sample is 28 x 28\n",
    "    '''\n",
    "    samples, H, W = X.shape\n",
    "    # Reshape input volume to (sample, 784), this mean, your NN input layer will have 784 placeholders\n",
    "    # we scale the RGB values by divide them by 255, this will help improve the training performance\n",
    "    return X.reshape(samples, H * W).T / 255\n",
    "\n",
    "def one_hot_vector(x, num_classes):\n",
    "    # By now, I think you already heard about this so many times\n",
    "    return np.eye(num_classes)[x].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3f093-b450-4bda-ae48-bfeeba9b0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join('data_hw2', 'fashion_mnist_npy')\n",
    "\n",
    "# The actual meaning of the label of your classes.\n",
    "# E.g. if an output one-hot vector is [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], it used to prepresent a Dress\n",
    "label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Load the training input\n",
    "X_train = np.load(os.path.join(PATH, 'train_data.npy'))\n",
    "# Load the training labels\n",
    "X_test = np.load(os.path.join(PATH, 'test_data.npy'))\n",
    "# Load the testing input\n",
    "Y_train = np.load(os.path.join(PATH, 'train_labels.npy'))\n",
    "# Load the testing labels\n",
    "Y_test = np.load(os.path.join(PATH, 'test_labels.npy'))\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(label_names)\n",
    "# Get the number of training samples and their resolution for reshape\n",
    "num_trains, HEIGHT, WIDTH = X_train.shape\n",
    "\n",
    "# Reshape the training and testing inputs\n",
    "X_train, X_test = reshape_train_data(X_train), reshape_train_data(X_test)\n",
    "\n",
    "# Create one-hot vector for the training and testing labels\n",
    "Y_train, Y_test = one_hot_vector(Y_train, num_classes), one_hot_vector(Y_test, num_classes)\n",
    "\n",
    "# This part use to randomly load some of the training and testing image and the one-hot vectors for checking\n",
    "fig_train, ax_train = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_train.suptitle(\"Random image from the TRAINING set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "fig_test, ax_test = plt.subplots(figsize=(16, 8), nrows=1, ncols=5)\n",
    "fig_test.suptitle(\"Random image from the TESTING set\", y=0.73, fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx in range(5):\n",
    "    i, j = np.random.randint(num_trains), np.random.randint(X_test.shape[0])\n",
    "    \n",
    "    ax_train[idx].imshow(X_train[:,i].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
    "    ax_train[idx].set_title(label_names[np.argmax(Y_train[:,i])] + \"\\n\" + str(Y_train[:,i]))\n",
    "    ax_train[idx].axis('off')\n",
    "    \n",
    "    ax_test[idx].imshow(X_test[:,j].reshape(HEIGHT, WIDTH), cmap = matplotlib.cm.binary)\n",
    "    ax_test[idx].set_title(label_names[np.argmax(Y_test[:,j])] + \"\\n\" + str(Y_test[:,j]))\n",
    "    ax_test[idx].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314d414-d7ee-4c04-99a3-e55cb70a5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of Y_train:', Y_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of Y_test:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c681a-4fbf-4ba0-835e-4ab8c970efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers of input units\n",
    "num_inputs = X_train.shape[0]\n",
    "\n",
    "# Number of neural in your hidden layer\n",
    "num_hidden_1 = 9\n",
    "num_hidden_2 = 3\n",
    "\n",
    "# Numbers of output units\n",
    "num_outputs = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aba680-05d3-4c84-b7c0-0afb91d3d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs):\n",
    "    \"\"\"Method for initialize the model parameters and the learning velocity of momentum for SGD.\n",
    "\n",
    "    Returns:\n",
    "        parameters (tuple): The model parameters: W1, W2, W3\n",
    "        velocity (tuple): The learning velocity of momentum: V_dW1, V_dW2, V_dW3\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct your neural network in Fig. 2 \n",
    "    # TODO: Random initialize the hidden_1, hidden_2 and output layer weights (w/o bias) (1 point)\n",
    "    # Hints: use np.random.randn()\n",
    "    W1 = \n",
    "    W2 = \n",
    "    W3 = \n",
    "\n",
    "    parameters = W1, W2, W3\n",
    "\n",
    "    # Zeros initialize the momentum for SGD\n",
    "    V_dW1 = np.zeros(W1.shape)\n",
    "    V_dW2 = np.zeros(W2.shape)\n",
    "    V_dW3 = np.zeros(W3.shape)\n",
    "\n",
    "    velocity = V_dW1, V_dW2, V_dW3\n",
    "\n",
    "    return parameters, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e6827-1329-49ce-8aad-0cbe69c19559",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, velocity = init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs)\n",
    "\n",
    "W1, W2, W3 = parameters\n",
    "\n",
    "print('Shape of W1:', W1.shape)\n",
    "print('Shape of W2:', W2.shape)\n",
    "print('Shape of W3:', W3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f601d2-da03-4b6b-9a0f-24080bbf7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "**<span style=\"color:green\">Reference Results:</span>** <br>\n",
    "Shape of W1: (9, 784)<br>\n",
    "Shape of W2: (3, 9)<br>\n",
    "Shape of W3: (10, 3)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e21d8-7699-4164-81f2-c640f2efae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO: implement Eq. 1 (1 point)\n",
    "    # Hints: use np.exp()\n",
    "    return \n",
    "\n",
    "def relu(X):\n",
    "    # TODO: implement Eq. 2 (1 point)\n",
    "    # Hints: use np.maximum()\n",
    "    return \n",
    "\n",
    "def softmax(X):\n",
    "    # TODO: implement Eq. 3 (1 point)\n",
    "    # Hints: use np.exp() and np.sum(, axis=0) <- beware of the axis\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210098b-9a7b-4ca7-8803-ea8338fd36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-1, 0, 2]).reshape(-1, 1)\n",
    "\n",
    "print('X')\n",
    "print(X, '\\n')\n",
    "\n",
    "print('sigmoid(x)')\n",
    "print(sigmoid(X), '\\n')\n",
    "\n",
    "print('relu(x)')\n",
    "print(relu(X), '\\n')\n",
    "\n",
    "print('softmax(x)')\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97943bd7-2212-4005-9bd5-627754481316",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "x = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "\n",
    "ax[0].set_title('Sigmoid')\n",
    "ax[0].plot(x, sigmoid(x))\n",
    "\n",
    "ax[1].set_title('ReLU')\n",
    "ax[1].plot(x, relu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d1fd7-cd82-4574-b382-3d3284e08777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"Method for forward propagation.\n",
    "    Args:\n",
    "        X (np.array): Input data\n",
    "        parameters (tuple): Parameters in the model: W1, W2, W3\n",
    "\n",
    "    Returns:\n",
    "        outputs (tuple): The forward propagation outputs: Z1, A1, Z2, A2, Z3, A3\n",
    "    \"\"\"\n",
    "\n",
    "    W1, W2, W3 = parameters\n",
    "    \n",
    "    # TODO: implement the forward propagation of Fig. 2 based on Eq. 4 - 9 (1 point)\n",
    "    # Hints: use np.matmul() to implement matrix multiplication\n",
    "    Z1 = \n",
    "    A1 = \n",
    "    Z2 = \n",
    "    A2 = \n",
    "    Z3 = \n",
    "    A3 = \n",
    "\n",
    "    outputs = Z1, A1, Z2, A2, Z3, A3\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e75514-69c6-44f7-8be9-33fac4626568",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(num_inputs, 2)\n",
    "\n",
    "parameters, _ = init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs)\n",
    "Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X, parameters)\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of Z1:', Z1.shape)\n",
    "print('Shape of A1:', A1.shape)\n",
    "print('Shape of Z2:', Z2.shape)\n",
    "print('Shape of A2:', A2.shape)\n",
    "print('Shape of Z3:', Z3.shape)\n",
    "print('Shape of A3:', A3.shape)\n",
    "\n",
    "print('\\nExample results of A3:')\n",
    "print(A3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09cb93-348d-4b30-b530-7259e32cd694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y, Y_pred):\n",
    "    # TODO: implement Eq. 10 (1 point)\n",
    "    # Hints: use np.sum(), np.multiply() and np.log()\n",
    "    # At the end, we need to divide by the number of of sample in the training batch e.g. m = Y.shape[1]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e832d-0fe0-4500-baa9-f2011be9230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "Y = one_hot_vector(np.random.randint(1, num_classes, 2), num_classes)\n",
    "A3 = softmax(np.random.randn(num_classes, 2))\n",
    "\n",
    "L = cross_entropy_loss(Y, A3)\n",
    "print('Shape of Y:', Y.shape)\n",
    "print('Shape of A3:', A3.shape)\n",
    "print('Loss:', L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235260c3-e4f0-4ca2-8433-5e6e3b3433c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, parameters, outputs):\n",
    "    \"\"\"Method for forward propagation.\n",
    "    Args:\n",
    "        X (np.array): Input data\n",
    "        Y (np.array): Ground truth of input data\n",
    "        parameters (tuple): Parameters in the model: W1, W2, W3\n",
    "        outputs (tuple): The forward propagation outputs: Z1, A1, Z2, A2, Z3, A3\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The computed gradients: dW1, dW2, dW3     \n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1] # The number of data samples in a mini batch\n",
    "\n",
    "    W1, W2, W3 = parameters\n",
    "    Z1, A1, Z2, A2, Z3, A3 = outputs\n",
    "    \n",
    "    # TODO: calculate the derivative of L with respect to Z3 and W3 using Eq. 14 - 15 (1 point)\n",
    "    dZ3 = \n",
    "    dW3 = \n",
    "\n",
    "    # TODO: calculate the derivative of L with respect to A2, Z2 and W2 using Eq. 16 - 19 (1 point)\n",
    "    dA2 = \n",
    "    dZ2 = \n",
    "    dW2 = \n",
    "\n",
    "    # TODO: calculate the derivative of L with respect to A1, Z1 and W1 using Eq. 20 - 23 (1 point)\n",
    "    dA1 = \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "\n",
    "    return dW1, dW2, dW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75a2a55-30a8-49c2-836d-78110f5a5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(num_inputs, 2)\n",
    "Y = np.random.randint(1, num_classes, (2, ))\n",
    "\n",
    "parameters, _ = init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs)\n",
    "outputs = forward_propagation(X, parameters)\n",
    "dW1, dW2, dW3 = backward_propagation(X, Y, parameters, outputs)\n",
    "\n",
    "print('Shape of dW3:', dW3.shape)\n",
    "print('Shape of dW2:', dW2.shape)\n",
    "print('Shape of dW1:', dW1.shape)\n",
    "\n",
    "print('\\nExample results in dW3:')\n",
    "print(dW3[:, 0])\n",
    "\n",
    "print('\\nExample results in dW2:')\n",
    "print(dW2[:, 0])\n",
    "\n",
    "print('\\nExample results in dW1:')\n",
    "print(dW1[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26ad65-fed3-4cc9-b1e4-f0aa862039d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(X, Y, parameters, velocity, learning_rate, beta, weight_decay_lambda=0.):\n",
    "    \"\"\"Method for each training step.\n",
    "    Args:\n",
    "        X (np.array): Input data\n",
    "        Y (np.array): Ground truth of input data\n",
    "        parameters (tuple): Parameters in the model: W1, W2, W3\n",
    "        velocity (tuple): The learning velocity of momentum: V_dW1, V_dW2, V_dW3\n",
    "        learning_rate (float): The learning rate for training\n",
    "        beta (float): The coefficient to update the learning velocity\n",
    "        weight_decay_lambda (float): The coefficient of weight decay\n",
    "\n",
    "    Returns:\n",
    "        tuple: loss, the updated parameters, the updated velocity\n",
    "    \"\"\"\n",
    "\n",
    "    W1, W2, W3 = parameters\n",
    "\n",
    "    # The forward propagation\n",
    "    outputs = forward_propagation(X, parameters)\n",
    "    A3 = outputs[-1]\n",
    "    \n",
    "    # Loss computation and backward propagation\n",
    "    if weight_decay_lambda == 0.:\n",
    "        loss = cross_entropy_loss(Y, A3)\n",
    "        dW1, dW2, dW3 = backward_propagation(X, Y, parameters, outputs)\n",
    "    else:\n",
    "        loss = cross_entropy_loss(Y, A3) + weight_decay_loss(parameters, weight_decay_lambda)\n",
    "        dW1, dW2, dW3 = backward_propagation_with_weight_decay(X, Y, parameters, outputs, weight_decay_lambda)\n",
    "    \n",
    "    V_dW1, V_dW2, V_dW3 = velocity\n",
    "    \n",
    "    # Updating model parameters\n",
    "    # TODO: Update the learning velocity using Eq. 28 (1 point)\n",
    "    V_dW1 = \n",
    "    V_dW2 = \n",
    "    V_dW3 = \n",
    "\n",
    "    # TODO: Update the model weights using Eq. 29 (1 point)    \n",
    "    W1 = \n",
    "    W2 = \n",
    "    W3 = \n",
    "\n",
    "    parameters = W1, W2, W3\n",
    "    velocity = V_dW1, V_dW2, V_dW3\n",
    "    \n",
    "    return loss, parameters, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5733c-81f5-4c2c-818d-47585a546f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(parameters, velocity, batch_size, epoch, learning_rate, beta, weight_decay_lambda=0.):\n",
    "    \n",
    "    # Calculate the number of training iterations base on the number of training samples and your batch size\n",
    "    num_batchs = num_trains // batch_size\n",
    "    print('Start training the model:')\n",
    "    print('Number of training samples: {}'.format(num_trains))\n",
    "    print('Number of batchs: {}'.format(num_batchs))\n",
    "\n",
    "    loss_log = [] # Log the training loss\n",
    "    \n",
    "    # Training\n",
    "    for i in range(epoch):    \n",
    "        start_t = time.time()\n",
    "\n",
    "        indices = np.random.permutation(num_trains)\n",
    "        X_train_shuffled, Y_train_shuffled = X_train[:, indices], Y_train[:, indices]\n",
    "\n",
    "        for j in range(num_batchs):\n",
    "\n",
    "            # Get mini-batch samples for training\n",
    "            start_idx, end_idx = j * batch_size, min(j * batch_size + batch_size, X_train.shape[1] - 1)\n",
    "            X, Y = X_train_shuffled[:, start_idx : end_idx], Y_train_shuffled[:, start_idx : end_idx]\n",
    "\n",
    "            # Call training_step()\n",
    "            loss, parameters, velocity = training_step(X, Y, parameters, velocity, learning_rate, beta, weight_decay_lambda)\n",
    "\n",
    "            loss_log.append(loss)\n",
    "            if (j > 0 and j % 200 == 0):\n",
    "                print(\"[Epoch][Iterations]:[{}/{}][{}/{}], loss: {}\".format(i, epoch, j, num_batchs, loss))\n",
    "\n",
    "        print(\"=> Epoch {}, elapsed time: {:.2f} seconds\".format(i, time.time() - start_t))\n",
    "    \n",
    "    plt.title('Training loss:')\n",
    "    plt.plot(loss_log)\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6461f67-c47b-4b4e-b412-1422b5785097",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, velocity = init_parameters(\n",
    "    num_inputs = X_train.shape[0], \n",
    "    num_hidden_1 = 9, \n",
    "    num_hidden_2 = 3,\n",
    "    num_outputs = num_classes,\n",
    ")\n",
    "\n",
    "trained_parameters = training_model(\n",
    "    parameters, \n",
    "    velocity,\n",
    "    batch_size = 128,                                     \n",
    "    epoch = 30,\n",
    "    learning_rate = 0.25,                                     \n",
    "    beta = 0.5,                                     \n",
    "    weight_decay_lambda = 0., \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677b13f-f21f-4ee5-abcb-61ad46834f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(X, parameters):\n",
    "    # TODO: implement the forward propagation for testing step (0.5 point)\n",
    "    # Hints: call forward_propagation() function and get the output A3\n",
    "    A3 = \n",
    "\n",
    "    # Evaluate the performance of your model\n",
    "    predictions = np.argmax(A3, axis=0)\n",
    "    labels = np.argmax(Y_test, axis=0)\n",
    "    \n",
    "    print(\"Confusion matrix:\\n{}\".format(confusion_matrix(labels, predictions)))\n",
    "    print(\"Testing accuracy: {}\".format(accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68644abf-d0e4-4cad-8747-9af3fc84711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model(X_test, trained_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a7d8d-2d07-425d-a019-81560c38e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_decay_loss(parameters, lambd):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        parameters (tuple): Parameters in the model: W1, W2, W3\n",
    "        lambd (float): The coefficient of weight decay\n",
    "        m (float): The number of data samples in a mini batch\n",
    "    \"\"\"\n",
    "    W1, W2, W3 = parameters\n",
    "    \n",
    "    # TODO: implement the weight decay loss in Eq. 32 (1 point)\n",
    "    # Hints: use np.sum() and np.power()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddad24-a148-42af-865b-32f63e8c12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "parameters, _ = init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs)\n",
    "\n",
    "loss = weight_decay_loss(parameters, lambd=0.0001)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060b576-8ed2-496a-be04-b92d2004bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_weight_decay(X, Y, parameters, outputs, lambd):\n",
    "    \"\"\"Method for forward propagation.\n",
    "    Args:\n",
    "        X (np.array): Input data\n",
    "        Y (np.array): Ground truth of input data\n",
    "        parameters (tuple): Parameters in the model: W1, W2, W3\n",
    "        outputs (tuple): The forward propagation outputs: Z1, A1, Z2, A2, Z3, A3\n",
    "        lambd (float): The coefficient of weight decay\n",
    "\n",
    "    Returns:\n",
    "        tuple: The computed gradients: dW1, dW2, dW3     \n",
    "    \"\"\"\n",
    "\n",
    "    W1, W2, W3 = parameters\n",
    "    dW1, dW2, dW3 = backward_propagation(X, Y, parameters, outputs)\n",
    "    \n",
    "    # TODO: calculate the new derivative of L with respect to W1, W2, and W3 based on Eq. 33 (1 point)\n",
    "    dW3_new = \n",
    "    dW2_new = \n",
    "    dW1_new = \n",
    "\n",
    "    return dW1_new, dW2_new, dW3_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5318439-ed87-493b-b11a-56f438f12c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(num_inputs, 2)\n",
    "Y = np.random.randint(1, num_classes, (2, ))\n",
    "\n",
    "parameters, _ = init_parameters(num_inputs, num_hidden_1, num_hidden_2, num_outputs)\n",
    "outputs = forward_propagation(X, parameters)\n",
    "dW1_new, dW2_new, dW3_new = backward_propagation_with_weight_decay(X, Y, parameters, outputs, lambd=0.0001)\n",
    "\n",
    "print('\\nExample results in dW3 (new):')\n",
    "print(dW3_new[:, 0])\n",
    "\n",
    "print('\\nExample results in dW2 (new):')\n",
    "print(dW2_new[:, 0])\n",
    "\n",
    "print('\\nExample results in dW1 (new):')\n",
    "print(dW1_new[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6be5f-b764-499d-8495-2e9069861e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the values of hyper-parameters below and try to improve the testing accuracy. (1 point)\n",
    "# Hints: try to adjust 'num_hidden_2', 'weight_decay_lambda' or other hyper-parameters\n",
    "\n",
    "parameters, velocity = init_parameters(\n",
    "    num_inputs = X_train.shape[0], \n",
    "    num_hidden_1 = 9, \n",
    "    num_hidden_2 = 3,\n",
    "    num_outputs=num_classes\n",
    ")\n",
    "\n",
    "trained_parameters = training_model(\n",
    "    parameters, \n",
    "    velocity,\n",
    "    batch_size = 128,                                     \n",
    "    epoch = 30,\n",
    "    learning_rate = 0.25,                                     \n",
    "    beta = 0.5,                                     \n",
    "    weight_decay_lambda = 0.0001, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef31f5f-a3ee-4b28-84a2-d7cedaef27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_model(X_test, trained_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
